[
  {
    "objectID": "Blog/about.html",
    "href": "Blog/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "Blog/index.html",
    "href": "Blog/index.html",
    "title": "Things that I would have found interesting 6 months ago",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Blog/posts/post-with-code/index.html",
    "href": "Blog/posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "Blog/posts/welcome/index.html",
    "href": "Blog/posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mohammad Amin Faraji",
    "section": "",
    "text": "Hi! I’m Mohammad Amin, a recent graduate from the University of Tehran with a master’s degree in mechanical engineering.\nMy research interests include optimizing lattice mechanical structures, applying machine learning in engineering, and using generative AI for the inverse design of mechanical structures. I’d rather be called “Nima” as I go by this among my friends!\nI’m passionate about using my knowledge to make a positive impact on society. In the future, I’d like to be committed to a Ph.D. in computer science or an interdisciplinary field involving machine learning, since this greatly impacts human lives. I’d like to have my own contributions.\nWhen I’m not working on my research, I enjoy playing chess on chess.com, reading Tim Urban’s blog, or listening to Lex Fridman’s podcasts.\nOn this site, you’ll find a list of my publications, notable projects, my CV, and a simple blog where I share my thoughts on computer science, life, and self-improvement.\n\n\n\n\n\nMulti Objective Optimization of a Bio-Inspired Structure in Non-Pneumatic Tires. (Under Review) Faraji, M.A, Shaban, M, Mazaheri, H. 2024. Advances in Engineering Software.\nStacked Ensemble Regression Model for Prediction of Furan. (https://doi.org/10.3390/en16227656). Faraji, M.A, Shooshtari, A, El-hag, A. 2023. Energies, MDPI.\nInvestigation on the characteristics of a Non-Pneumatic tire with different spoke shapes. (Link) Faraji, M.A, Daneshmehr, A.R. 2022. The 30th Annual International Conference of Iranian Society of Mechanical Engineers-ISME2022.\nComputational study on a DAH auxetic structure manufactured by corrugated sheets. (Link) Faraji, M.A, Mazaheri, H. 2020. International Conference on Manufacturing Engineering at Tarbiat Modares University-ICME 2018.\n\n\n\nComprehensive Study on Feature Importance of Transformers Test Data:\nExtracting feature importance of transformers test data and examining their dependencies.\nDebiasing:\nThis code is part of the MIT intro to deep learning course.\nU.S. Patent Phrase to Phrase Matching:\nMy code for Kaggle’s NLP competition “U.S. Patent Phrase to Phrase Matching”.\nPen or Pencil classifier:\nIt’s whole objective is a web deployment of a pen or a pencil classifier that is trained on the pictures from the internet."
  },
  {
    "objectID": "pdf/pdf.html",
    "href": "pdf/pdf.html",
    "title": "pdf",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "pdf/transformer-paper4.html",
    "href": "pdf/transformer-paper4.html",
    "title": "",
    "section": "",
    "text": "::: {.cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2023-05-18T13:45:05.122442Z”,“iopub.status.busy”:“2023-05-18T13:45:05.122109Z”,“iopub.status.idle”:“2023-05-18T13:45:05.152481Z”,“shell.execute_reply”:“2023-05-18T13:45:05.151239Z”,“shell.execute_reply.started”:“2023-05-18T13:45:05.122418Z”}’ trusted=‘true’ execution_count=1}\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n/kaggle/input/transformer/DatasetB.csv\n/kaggle/input/transformer/DatasetA.csv\n\n:::\nHere, we have loaded the data and set Furan as the label. At first, we have used 25 percent of the dataset A as the test set to come up with a good model, and then use this model to test in the dataset B.\n\nds_A = pd.read_csv(\"/kaggle/input/transformer/DatasetA.csv\")\nds_B = pd.read_csv(\"/kaggle/input/transformer/DatasetB.csv\")\n\n# Splitting train and test\nfrom sklearn.model_selection import train_test_split\ntrain_set_A, test_set_A = train_test_split(ds_A, test_size = 0.25, random_state = 11)\n\n# Setting the labels\ny_train_A = train_set_A['Furan']\ny_test_A = test_set_A['Furan']\n\n# Dropping the Furan and Health Index columns\nX_train_A = train_set_A.drop([\"Furan\", \"HI\"], axis = 1)\nX_test_A = test_set_A.drop([\"Furan\", \"HI\"], axis = 1)\n\n# For DatasetB\ny_B = ds_B['Furan']\nX_B = ds_B.drop([\"Furan\", \"HI\"], axis = 1)\n\n# The code below is for the second case, where we train the data for the whole\n# Dataset A and test it on Dataset B\ny_A = ds_A['Furan']\nX_A = ds_A.drop([\"Furan\", \"HI\"], axis = 1)\n\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n\n\nThe code below, drops the columns that we don’t need, and only keeps the common features between dataset A and B.\n\nX_train_A = X_train_A.drop(set(ds_A.columns) - set(ds_B.columns), axis=1)\nX_test_A = X_test_A.drop(set(ds_A.columns) - set(ds_B.columns), axis=1)\nX_A = X_A.drop(set(ds_A.columns) - set(ds_B.columns), axis=1)\nX_B = X_B[X_train_A.columns]\nX_train_A\n\n\n\n\n\n  \n    \n      \n      H2\n      Methane\n      Acetylene\n      Ethylene\n      Ethane\n      Water\n      Acid\n      BDV\n      IFT\n    \n  \n  \n    \n      109\n      12.2\n      53.50\n      6.9\n      127.4\n      48.0\n      3\n      0.043\n      83.0\n      20\n    \n    \n      566\n      30.2\n      0.00\n      0.0\n      2.6\n      1.1\n      3\n      0.005\n      84.0\n      39\n    \n    \n      410\n      45.6\n      18.20\n      0.0\n      1.6\n      1.7\n      5\n      0.005\n      87.0\n      30\n    \n    \n      316\n      19.7\n      38.50\n      0.0\n      2.7\n      41.6\n      7\n      0.005\n      50.0\n      32\n    \n    \n      678\n      11.0\n      7.60\n      0.0\n      0.3\n      1.6\n      3\n      0.005\n      61.0\n      42\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      13.7\n      5.10\n      0.0\n      0.4\n      1.1\n      1\n      0.005\n      94.0\n      36\n    \n    \n      337\n      32.9\n      3.77\n      0.0\n      0.6\n      2.4\n      6\n      0.005\n      79.0\n      32\n    \n    \n      91\n      22.8\n      3.30\n      0.0\n      4.9\n      3.0\n      11\n      0.140\n      88.0\n      16\n    \n    \n      80\n      61.2\n      27.30\n      0.0\n      25.6\n      20.8\n      9\n      0.099\n      70.0\n      17\n    \n    \n      703\n      58.1\n      9.40\n      0.0\n      1.4\n      1.9\n      5\n      0.005\n      86.0\n      33\n    \n  \n\n547 rows × 9 columns\n\n\n\nThe code below, discretizes the Furan data into 3 classes.\n\n# define the bin edges for each class\nbins = [-1, 0.1, 1, 100]\n\n# define the labels for each class\nlabels = [0, 1, 2]\n\ny_train_A = pd.DataFrame(y_train_A)\ny_B = pd.DataFrame(y_B)\ny_test_A = pd.DataFrame(y_test_A)\ny_A = pd.DataFrame(y_A)\n\n# discretize the data into 3 classes\ny_train_A['Class'] = pd.cut(y_train_A['Furan'], bins=bins, labels=labels)\ny_B['Class'] = pd.cut(y_B['Furan'], bins=bins, labels=labels)\ny_test_A['Class'] = pd.cut(y_test_A['Furan'], bins=bins, labels=labels)\ny_A['Class'] = pd.cut(y_A['Furan'], bins=bins, labels=labels)\n\ny_train_A = np.array(y_train_A.drop(\"Furan\", axis = 1)).ravel()\ny_B = np.array(y_B.drop(\"Furan\", axis = 1)).ravel()\ny_test_A = np.array(y_test_A.drop(\"Furan\", axis = 1)).ravel()\ny_A = np.array(y_A.drop(\"Furan\", axis = 1)).ravel()\n\nThe below code is a function to plot the confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\ndef plot_confusion_matrix(cm, classes, normalize=False, cmap=plt.cm.Blues, title='Confusion matrix'):\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n\nFirst case: Training using 75% of the data and testing on the remaining 25%\nWe have experimented a combination of different models in the ensemble. Although the results were quite similar, we found that a combination of KNN, XGB and logistic regression works best. In the code below we have created a voting classifier consist of these models.\n\n# from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\n\n#log_clf = LogisticRegression(max_iter=1000)\nsvm_clf = SVC(probability=True, gamma=0.001)\nknn_clf = KNeighborsClassifier(n_neighbors=3)\nxgb_clf = XGBClassifier(learning_rate=0.01, n_estimators=300, max_depth=3, subsample=0.7)\nmlp_clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)\nnb_clf = GaussianNB()\nada_clf = AdaBoostClassifier(n_estimators=50, learning_rate=0.003)\nlr_clf = LogisticRegression(max_iter=10000)\n\nvoting_clf = VotingClassifier(\n  estimators=[#('nn', mlp_clf),\n              #('svc', svm_clf),\n              ('knn', knn_clf), #('ada', ada_clf),('nb', nb_clf)\n              ('xgb', xgb_clf), ('lr', lr_clf)],\n  voting='hard')\nvoting_clf.fit(X_train_A, np.array(y_train_A).ravel())\n\nVotingClassifier(estimators=[('knn', KNeighborsClassifier(n_neighbors=3)),\n                             ('xgb',\n                              XGBClassifier(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=None,\n                                            early_stopping_rounds=None,\n                                            enable_categorical=False,\n                                            eval_metric=None,\n                                            feature_types=None, gamma=None,\n                                            gpu_id=None, grow_policy=None,\n                                            importance_type=None,\n                                            interaction_constraints=None,\n                                            learning_rate=0.01, max_bin=None,\n                                            max_cat_threshold=None,\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=3,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            n_estimators=300, n_jobs=None,\n                                            num_parallel_tree=None,\n                                            predictor=None, random_state=None, ...)),\n                             ('lr', LogisticRegression(max_iter=10000))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.VotingClassifierVotingClassifier(estimators=[('knn', KNeighborsClassifier(n_neighbors=3)),\n                             ('xgb',\n                              XGBClassifier(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=None,\n                                            early_stopping_rounds=None,\n                                            enable_categorical=False,\n                                            eval_metric=None,\n                                            feature_types=None, gamma=None,\n                                            gpu_id=None, grow_policy=None,\n                                            importance_type=None,\n                                            interaction_constraints=None,\n                                            learning_rate=0.01, max_bin=None,\n                                            max_cat_threshold=None,\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=3,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            n_estimators=300, n_jobs=None,\n                                            num_parallel_tree=None,\n                                            predictor=None, random_state=None, ...)),\n                             ('lr', LogisticRegression(max_iter=10000))])knnKNeighborsClassifierKNeighborsClassifier(n_neighbors=3)xgbXGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=3, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)lrLogisticRegressionLogisticRegression(max_iter=10000)\n\n\nHere is a comparison of different models and the voting classifier.\n\nfrom sklearn.metrics import accuracy_score\nfor clf in (mlp_clf, svm_clf, #ada_clf,\n            knn_clf, xgb_clf, #nb_clf,\n            lr_clf, voting_clf):\n    clf.fit(X_train_A, y_train_A)\n    y_pred_A = clf.predict(X_test_A)\n    y_pred_B = clf.predict(X_B)\n    print(clf.__class__.__name__ + \" for dataset A:\", accuracy_score(y_test_A, y_pred_A))\n    print(clf.__class__.__name__ + \" for dataset B:\", accuracy_score(y_B, y_pred_B))\n\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n  warnings.warn(\n\n\nMLPClassifier for dataset A: 0.8688524590163934\nMLPClassifier for dataset B: 0.7828746177370031\nSVC for dataset A: 0.8797814207650273\nSVC for dataset B: 0.8103975535168195\nKNeighborsClassifier for dataset A: 0.8360655737704918\nKNeighborsClassifier for dataset B: 0.8042813455657493\nXGBClassifier for dataset A: 0.8961748633879781\nXGBClassifier for dataset B: 0.764525993883792\nLogisticRegression for dataset A: 0.8633879781420765\nLogisticRegression for dataset B: 0.7951070336391437\nVotingClassifier for dataset A: 0.8961748633879781\nVotingClassifier for dataset B: 0.8195718654434251\n\n\n\nclass_names = ['A', 'B', 'C']\nvoting_clf.fit(X_train_A, y_train_A)\ny_pred_A = clf.predict(X_test_A)\ncnf_matrix = confusion_matrix(y_test_A, y_pred_A)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names,\n                      title='Confusion matrix for ' + clf.__class__.__name__)\nplt.show()\n\nConfusion matrix, without normalization\n[[130   0   2]\n [  4  11  11]\n [  1   1  23]]\n\n\n\n\n\n\n\nSecond case: Training using all of the data from Dataset A\nSo far we have used 75% of Dataset A to train the data and 25% to test it. Here, we used all of the data from Dataset A to train, and then test it on Dataset B.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\n\nlr_clf = LogisticRegression(max_iter=10000)\nsvm_clf = SVC(probability=True)\nknn_clf = KNeighborsClassifier()\nxgb_clf = XGBClassifier(learning_rate=0.01, n_estimators=300, max_depth=3, subsample=0.7)\nmlp_clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)\nnb_clf = GaussianNB()\n#ada_clf = AdaBoostClassifier(n_estimators=50, learning_rate=0.003)\n\nvoting_clf = VotingClassifier(\n  estimators=[#('nn', mlp_clf),\n              #('svc', svm_clf),\n              ('knn', knn_clf), #('ada', ada_clf),('nb', nb_clf)\n              ('xgb', xgb_clf), ('lr', lr_clf)],\n  voting='hard')\nvoting_clf.fit(X_A, y_A)\n\nVotingClassifier(estimators=[('knn', KNeighborsClassifier()),\n                             ('xgb',\n                              XGBClassifier(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=None,\n                                            early_stopping_rounds=None,\n                                            enable_categorical=False,\n                                            eval_metric=None,\n                                            feature_types=None, gamma=None,\n                                            gpu_id=None, grow_policy=None,\n                                            importance_type=None,\n                                            interaction_constraints=None,\n                                            learning_rate=0.01, max_bin=None,\n                                            max_cat_threshold=None,\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=3,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            n_estimators=300, n_jobs=None,\n                                            num_parallel_tree=None,\n                                            predictor=None, random_state=None, ...)),\n                             ('lr', LogisticRegression(max_iter=10000))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.VotingClassifierVotingClassifier(estimators=[('knn', KNeighborsClassifier()),\n                             ('xgb',\n                              XGBClassifier(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=None,\n                                            early_stopping_rounds=None,\n                                            enable_categorical=False,\n                                            eval_metric=None,\n                                            feature_types=None, gamma=None,\n                                            gpu_id=None, grow_policy=None,\n                                            importance_type=None,\n                                            interaction_constraints=None,\n                                            learning_rate=0.01, max_bin=None,\n                                            max_cat_threshold=None,\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=3,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            n_estimators=300, n_jobs=None,\n                                            num_parallel_tree=None,\n                                            predictor=None, random_state=None, ...)),\n                             ('lr', LogisticRegression(max_iter=10000))])knnKNeighborsClassifierKNeighborsClassifier()xgbXGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=3, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)lrLogisticRegressionLogisticRegression(max_iter=10000)\n\n\n\nfrom sklearn.metrics import accuracy_score\n\nfor clf in (#mlp_clf, svm_clf, #ada_clf,\n            knn_clf, xgb_clf, lr_clf, #nb_clf,\n            voting_clf):\n    clf.fit(X_A, y_A)\n    y_pred_B = clf.predict(X_B)\n    print(clf.__class__.__name__ + \" for dataset B:\", accuracy_score(y_B, y_pred_B))\n\nKNeighborsClassifier for dataset B: 0.8379204892966361\nXGBClassifier for dataset B: 0.7706422018348624\nLogisticRegression for dataset B: 0.8134556574923547\nVotingClassifier for dataset B: 0.8256880733944955\n\n\nThe code below is a function to visualize the confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\ndef plot_confusion_matrix(cm, classes, normalize=False, cmap=plt.cm.Blues, title='Confusion matrix'):\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nConfusion Matrix:\n\nclass_names = ['A', 'B', 'C']\n\nfor clf in (#mlp_clf, svm_clf, #ada_clf,\n            knn_clf, xgb_clf, lr_clf, #nb_clf,\n            voting_clf):\n    clf.fit(X_A, y_A)\n    y_pred_B = clf.predict(X_B)\n    cnf_matrix = confusion_matrix(y_B, y_pred_B)\n    np.set_printoptions(precision=2)\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=class_names,\n                          title='Confusion matrix for ' + clf.__class__.__name__)\n    plt.show()\n\nConfusion matrix, without normalization\n[[245   8   4]\n [ 11  11  18]\n [  8   4  18]]\n\n\n\n\n\nConfusion matrix, without normalization\n[[222  20  15]\n [  6   3  31]\n [  3   0  27]]\n\n\n\n\n\nConfusion matrix, without normalization\n[[235  17   5]\n [ 10   7  23]\n [  5   1  24]]\n\n\n\n\n\nConfusion matrix, without normalization\n[[242  10   5]\n [ 10   3  27]\n [  5   0  25]]\n\n\n\n\n\n\nclass_names = ['A', 'B', 'C']\nvoting_clf.fit(X_A, y_A)\ny_pred_B = voting_clf.predict(X_B)\ncnf_matrix = confusion_matrix(y_B, y_pred_B)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names,\n                      title='Confusion matrix for ' + clf.__class__.__name__)\nplt.show()\n\nConfusion matrix, without normalization\n[[242  10   5]\n [ 10   3  27]\n [  5   0  25]]"
  }
]